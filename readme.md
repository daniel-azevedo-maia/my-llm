# ü§ñ Sistema LLM Local - Projeto Pessoal de Daniel Maia

Este √© meu projeto pessoal de Intelig√™ncia Artificial Local, constru√≠do para estudar e experimentar a constru√ß√£o de sistemas de IA privados, que funcionem 100% offline, com alta performance e controle total dos dados.

---

## üìå O que o projeto faz?

O sistema permite carregar documentos pessoais (PDF, DOCX, TXT) e, a partir deles, o sistema "aprende" seu conte√∫do, criando uma **base de conhecimento vetorial**.\
Com isso, posso fazer perguntas em linguagem natural e o sistema responde baseado nesses documentos.

Ou seja, √© um sistema privado de **Perguntas e Respostas sobre documentos pessoais** com IA local.

---

## üìå √â RAG? √â Fine-Tuning?

Este projeto implementa um sistema de **RAG (Retrieval Augmented Generation)** e **n√£o usa Fine-Tuning**.

- üîé **RAG (Busca + Gera√ß√£o)**\
  Ele n√£o altera o modelo de linguagem. Ao inv√©s disso, recupera trechos relevantes dos documentos (usando embeddings vetoriais) e os injeta como contexto no prompt enviado ao modelo.\
  Assim o modelo responde de forma contextualizada.

- üß™ **Fine-tuning (n√£o utilizado)**\
  N√£o realizo nenhum ajuste no modelo LLM. O modelo permanece intacto.

Esta abordagem (RAG) √© altamente eficiente para sistemas de perguntas e respostas baseados em documentos.

---

## üìå Tecnologias e bibliotecas utilizadas

| Biblioteca                           | Fun√ß√£o                                                      |
| ------------------------------------ | ----------------------------------------------------------- |
| **Python 3.7+**                      | Linguagem principal                                         |
| **Tkinter**                          | Interface gr√°fica desktop                                   |
| **Ollama**                           | Servidor local que executa o LLM                            |
| **Llama 3.1 (8B)**                   | Modelo de linguagem (LLM)                                   |
| **ChromaDB**                         | Banco de dados vetorial local (armazenamento de embeddings) |
| **PyPDF2**                           | Leitura de arquivos PDF                                     |
| **python-docx**                      | Leitura de arquivos DOCX                                    |
| **requests**                         | Comunica√ß√£o HTTP com o Ollama                               |
| **subprocess / logging / threading** | Controle de processos, logs e execu√ß√£o paralela             |

---

## üìå Estrutura de armazenamento

| Tipo de dado             | Local de armazenamento                                      |
| ------------------------ | ----------------------------------------------------------- |
| C√≥digo fonte             | Pasta do projeto                                            |
| Depend√™ncias Python      | Diret√≥rio padr√£o do pip (`--user`)                          |
| Base vetorial ChromaDB   | Subpasta local do projeto (`/chroma_db/`)                   |
| Modelo Llama 3.1 baixado | Diret√≥rio interno do Ollama (`C:\Users\<usuario>\.ollama\`) |
| Logs e hist√≥rico de chat | Em mem√≥ria (n√£o persistido em disco)                        |

---

## üìå Fluxo completo de instala√ß√£o e execu√ß√£o (ORDEM EXATA)

### 1Ô∏è‚É£ Pr√©-requisitos manuais iniciais

**A. Instalar Python 3.7+**

- Acesse: [https://python.org/downloads](https://python.org/downloads)
- Instale o Python.
- Marque a op√ß√£o "Add Python to PATH" durante a instala√ß√£o.

**B. Instalar Ollama (Windows)**

- Acesse: [https://ollama.com/download](https://ollama.com/download)
- Baixe e instale o Ollama para Windows.
- Este passo instala o servidor Ollama local.

**C. (Opcional) Criar a pasta do projeto**

Exemplo:

```
C:\MeusProjetos\SISTEMA_LLM_LOCAL\
```

---

### 2Ô∏è‚É£ Executar o script de instala√ß√£o autom√°tica `.bat`

Agora execute o arquivo `instalador.bat` inclu√≠do no projeto:

```bat
@echo off
REM Script de instala√ß√£o para Windows
echo ü§ñ Sistema LLM Local - Instala√ß√£o Windows

REM Verifica Python
python --version >nul 2>&1
if errorlevel 1 (
    echo ‚ùå Python n√£o encontrado!
    echo Por favor, instale Python 3.7+ de: https://python.org/downloads
    pause
    exit /b 1
)
echo ‚úÖ Python encontrado

REM Instala depend√™ncias
echo üì¶ Instalando depend√™ncias Python...
python -m pip install --user PyPDF2 python-docx chromadb requests

REM Verifica Ollama
ollama --version >nul 2>&1
if errorlevel 1 (
    echo ‚ö†Ô∏è Ollama n√£o encontrado
    echo Por favor, baixe e instale Ollama de: https://ollama.com/download
    pause
    exit /b 1
)
echo ‚úÖ Ollama encontrado

REM Baixa modelo
echo üß† Baixando modelo Llama 3.1 (isso pode demorar)...
ollama pull llama3.1

echo ‚úÖ Instala√ß√£o conclu√≠da!
echo Para iniciar: python main.py
pause
```

Esse script:

- Instala as depend√™ncias Python
- Verifica o Ollama
- Faz o download do modelo com:

```bash
ollama pull llama3.1
```

O modelo fica armazenado localmente no cache do Ollama.

---

### 3Ô∏è‚É£ Rodar o sistema normalmente

Depois da instala√ß√£o, para iniciar o sistema execute:

```bash
python main.py
```

O sistema:

- Detecta que tudo est√° instalado.
- Inicializa o servidor Ollama.
- Abre a interface gr√°fica (Tkinter).
- J√° est√° pronto para uso.

---

### 4Ô∏è‚É£ (Opcional) Reexecutar o setup completo via terminal

Se desejar rodar o setup direto pelo pr√≥prio c√≥digo:

```bash
python main.py --setup
```

---

## üìå Como funciona o sistema

1Ô∏è‚É£ O usu√°rio adiciona documentos (PDF, DOCX, TXT).

2Ô∏è‚É£ O sistema:

- L√™ o conte√∫do.
- Divide em "chunks" (trechos).
- Gera embeddings vetoriais.
- Armazena localmente via ChromaDB.

3Ô∏è‚É£ Quando uma pergunta √© feita:

- Busca vetorial local (via ChromaDB) encontra trechos relevantes.
- O contexto √© injetado no prompt.
- O prompt √© enviado ao modelo Llama 3.1 no Ollama.
- A resposta √© gerada e exibida.

---

## üìå Por que este projeto n√£o usa LangChain ou LlamaIndex?

- Toda a l√≥gica de ingest√£o de documentos e fluxo RAG foi implementada manualmente, para maior controle e aprendizado.
- O projeto n√£o depende de frameworks externos.
- 100% open-source, did√°tico e transparente.

---

## üìå Poss√≠veis melhorias futuras

- Melhorar o splitting dos documentos (divis√£o sem√¢ntica).
- Utilizar embeddings ainda mais modernos.
- Implementar threads de conversa com mem√≥ria.
- Adicionar backup autom√°tico da base vetorial.
- Melhorar ainda mais a interface gr√°fica.

---

‚úÖ **Projeto 100% funcional, local, privativo e controlado.**

Desenvolvido e mantido por:

**Daniel Maia**

---

